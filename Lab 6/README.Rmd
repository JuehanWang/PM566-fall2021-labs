---
title: "Lab 5"
author: "Juehan Wang"
date: "9/24/2021"
output:
    html_document:
      toc: yes 
      toc_float: yes 
      keep_md: yes
    github_document:
      keep_html: true
      html_preview: false
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(data.table)
library(tidyverse)
library(tidytext)
library(tibble)
library(dplyr)
```

Some Notes:

  Add README files -- git add "Lab 6/README*"
  
  Remove cache files before committing -- git rm --cache "Lab 6/README_cache*"

First, download the data.

```{r get-data}
fn <- "mtsamples.csv"
if (!file.exists(fn))
  download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv", destfile = fn)

mtsamples<-read.csv(fn)
mtsamples<-as_tibble(mtsamples)
```

### Question 1: What specialties do we have?

We can use count() from dplyr to figure out how many different categories do we have? Are these categories related? overlapping? evenly distributed?

```{r dist-of-specialists}
specialties <- mtsamples %>%
  count(medical_specialty, sort = TRUE)
```

There are `r nrow(specialties)` specialties. Let's take a look at the distributions.

```{r dist1}
ggplot(mtsamples, aes(x = medical_specialty)) +
  geom_histogram(stat = "count") +
  coord_flip()

specialties %>%
  arrange(desc(n)) %>%
  top_n(n, 15) %>%
  knitr::kable()
```

```{r dist2}
ggplot(specialties, aes(x = n, y = fct_reorder(medical_specialty,n))) +
  geom_col()
```

These are not evenly (uniformly) distributed.

### Question 2

Tokenize the the words in the transcription column

Count the number of times each token appears

Visualize the top 20 most frequent words

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r token-trans, cache=TRUE}
mtsamples %>%
  unnest_tokens(output = word, input = transcription) %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(word,n))) +
    geom_col()
```

The word "status" seems to be important (duh!), but we observe a lot of stopwords.

### Question 3

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

```{r token-trans-wo-stop, cache=TRUE}
# Redo visualization but remove stopwords before
mtsamples %>%
  unnest_tokens(output = word, input = transcription) %>%
  count(word, sort = TRUE) %>%
  anti_join(stop_words, by = "word") %>%
  # using regular expressions to remove numbers
  filter(!grepl(pattern = "^[0-9]+$", x = word)) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(word,n))) +
    geom_col()
```

### Question 4

repeat question 2, but this time tokenize into bi-grams. how does the result change if you look at tri-grams?

```{r bigram-trans, cache=TRUE}
mtsamples %>%
  unnest_ngrams(output = bigram, input = transcription, n = 2) %>%
  count(bigram, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(bigram,n))) +
    geom_col()
```

Using bi-grams is not very informative, let's try tri-grams.

```{r trigram-trans, cache=TRUE}
mtsamples %>%
  unnest_ngrams(output = trigram, input = transcription, n = 3) %>%
  count(trigram, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(x = n, y = fct_reorder(trigram,n))) +
    geom_col()
```

Now some phrases start to show up, e.g. "tolerated the procedure", "prepped and draped".

### Question 5

Using the results you got from questions 4. Pick a word and count the words that appears after and before it.

```{r status, cache=TRUE, warning=FALSE}
bigrams <- mtsamples %>%
  unnest_ngrams(output = bigram, input = transcription, n = 3) %>%
  separate(bigram, into = c("w1", "w2"), sep = " ")

# before
bigrams %>%
  filter(w1 == "status") %>%
  select(w1,w2) %>%
  count(w2, sort = TRUE)

# after
bigrams %>%
  filter(w2 == "status") %>%
  select(w1,w2) %>%
  count(w1, sort = TRUE)
```

Since we are looking at single words again, it is a good idea to treat these as single tokens. So let's rename the stopwords and the numbers.

```{r status-wo-stop}
bigrams %>%
  filter(w1 == "status") %>%
  filter(!(w2 %in% stop_words$word) & !grepl(pattern = "^[0-9]+$", w2)) %>%
  count(w2, sort = TRUE) %>%
  top_n(10) %>%
  knitr::kable(caption = "Words AFTER 'status'")

bigrams %>%
  filter(w2 == "status") %>%
  filter(!(w1 %in% stop_words$word) & !grepl(pattern = "^[0-9]+$", w1)) %>%
  count(w1, sort = TRUE) %>%
  top_n(10) %>%
  knitr::kable(caption = "Words BEFORE 'status'")
```

### Question 6

Which words are most used in each of the specialties. you can use group_by() and top_n() from dplyr to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r top-per-specialty}
mtsamples %>%
  unnest_tokens(word, input = transcription) %>%
  group_by(medical_specialty) %>%
  count(word, sort = TRUE) %>%
  filter(!(word %in% stop_words$word) & !grepl("^[0-9]+$", word)) %>%
  top_n(5) %>%
  arrange(medical_specialty, n) %>%
  knitr::kable()
```





### Question 7 - extra

Find your own insight in the data:

Ideas:

Interesting ngrams
See if certain words are used more in some specialties then others

```{r}

```



